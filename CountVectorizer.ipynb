{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ujjwal29/sentiment-Analysis/blob/master/CountVectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxPBVtjkmtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "6a162ac6-4200-4005-92bc-6012084c4f1b"
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV5odeRbkmt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnCk3h8qkmuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUI3ix3kmum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5ec25bc8-e489-4b3a-c3ad-bf991218caac"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>sentiment_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.245025e+18</td>\n",
              "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>BeenXXPired</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.245759e+18</td>\n",
              "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
              "      <td>en</td>\n",
              "      <td>1</td>\n",
              "      <td>FestiveFeeling</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.246087e+18</td>\n",
              "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>KrisAllenSak</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.244803e+18</td>\n",
              "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>Queenuchee</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.244876e+18</td>\n",
              "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
              "      <td>en</td>\n",
              "      <td>0</td>\n",
              "      <td>brittan17446794</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  ... sentiment_class\n",
              "0  1.245025e+18  ...               0\n",
              "1  1.245759e+18  ...               0\n",
              "2  1.246087e+18  ...              -1\n",
              "3  1.244803e+18  ...               0\n",
              "4  1.244876e+18  ...              -1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9olJQix1kmuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['id','original_author', 'lang', 'retweet_count'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2w9wFK5kmu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e33146c4-42dd-4c0d-ef08-af6bc56bd795"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>sentiment_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  sentiment_class\n",
              "0  Happy #MothersDay to all you amazing mothers o...                0\n",
              "1  Happy Mothers Day Mum - I'm sorry I can't be t...                0\n",
              "2  Happy mothers day To all This doing a mothers ...               -1\n",
              "3  Happy mothers day to this beautiful woman...ro...                0\n",
              "4  Remembering the 3 most amazing ladies who made...               -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckWkUv74kmvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56676094-dd0f-415d-8313-879956f2c8cd"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3235, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMNfZTgEkmvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d2b1e62-d0e1-4786-93b8-e1b40ecacb59"
      },
      "source": [
        "df.sentiment_class.isnull().any()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-G08bykmvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ce0505d-890a-4d0d-e614-b92f2144ec57"
      },
      "source": [
        "df.original_text.isnull().any()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgyyLb_-kmvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.loc[df['original_text'].notna(), :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX1KyUNLkmvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = df['original_text']\n",
        "labels = df['sentiment_class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAerHE7Qkmvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "50d2a1a5-ad65-49d2-cab7-5167951bf57e"
      },
      "source": [
        "#tokenize\n",
        "text = text.apply(nltk.word_tokenize)\n",
        "print('Tokenized')\n",
        "\n",
        "#Remove stop words\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "text = text.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "print('Stop words Removed')\n",
        "\n",
        "\n",
        "#Remove Punctuation\n",
        "regex = '[a-z]+'\n",
        "text = text.apply(lambda x: [item for item in x if re.match(regex, item)])\n",
        "print('Punctuation Removed')\n",
        "\n",
        "#Lemmetization\n",
        "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "text = text.apply(lambda x: [lem.lemmatize(item, pos='v') for item in x])\n",
        "print('Lemmetize')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized\n",
            "Stop words Removed\n",
            "Punctuation Removed\n",
            "Lemmetize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6A0Pj-5kmvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yye2sRgrkmv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = [\" \".join(sen) for sen in text.values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA7rvvcQkmv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(vec, x_train, x_test):\n",
        "    x_train_vec = vec.fit_transform(x_train)\n",
        "    x_test_vec = vec.transform(x_test)\n",
        "    \n",
        "    print(\"Vectorization Complete!\")\n",
        "    \n",
        "    return x_train_vec, x_test_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndhJj6Ukmv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd1f1ead-0d2e-4550-d82d-62a4d8c32ffd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_train_vec, X_test_vec = vectorize(CountVectorizer(), X_train, X_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYNlqvpEkmwD",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ray_TqelkmwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the model\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "# param = { \n",
        "#     'learning_rate': [0.05, 0.1, 0.01], \n",
        "#     'min_samples_split': [2, 5, 7, 10],\n",
        "#     'n_estimators':[50, 100, 150, 200, 400, 500],\n",
        "#     'max_features':[5,10,15,20,25],\n",
        "#     'max_depth':[3,5,10,20,25,30]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrynXlVnkmwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "0e2a833c-4d74-4fd2-c6fd-dd711859842d"
      },
      "source": [
        "gbc = GradientBoostingClassifier(learning_rate=0.05, \n",
        "                                 max_depth=3, \n",
        "                                 max_features=10, \n",
        "                                 min_samples_split=7, \n",
        "                                 n_estimators=500)\n",
        "gbc.fit(X_train_vec, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.05, loss='deviance', max_depth=3,\n",
              "                           max_features=10, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=7,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGTpFzGtkmwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "c3aed943-db43-4146-f2f7-1af5afdd915f"
      },
      "source": [
        "#Predictions\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "y_pred = gbc.predict(X_test_vec)\n",
        "\n",
        "print(\"Precision: %1.3f \" % (precision_score(y_test, y_pred, average='macro')))\n",
        "print(\"Recall: %1.3f \" %  recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1: %1.3f\" % f1_score(y_test, y_pred, average='macro'))\n",
        "scores = cross_val_score(gbc, X_test_vec, y_test, cv=5)\n",
        "print(\"Score Mean: %1.3f\" % (scores.mean()))\n",
        "print(\"GBoosting Score Accuracy\", str(round(accuracy_score(y_pred,y_test)*100, 2)) + \"%\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.165 \n",
            "Recall: 0.333 \n",
            "F1: 0.221\n",
            "Score Mean: 0.481\n",
            "GBoosting Score Accuracy 49.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J88jWYCpkmwa",
        "colab_type": "text"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD1xE8qjkmwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA62J5ySkmwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lgbm = lightgbm.LGBMClassifier(num_leaves=80,\n",
        "#                               max_depth=-1,\n",
        "#                               learning_rate = 0.05,\n",
        "#                               n_estimators=8,\n",
        "#                               n_jobs=-1,\n",
        "#                               reg_alpha=0.0)\n",
        "\n",
        "# lgbm.fit(X_train_vec, y_train)\n",
        "\n",
        "# y_pred = lgbm.predict(X_test_vec)\n",
        "\n",
        "# print(\"LGBM Score Accuracy\", str(round(accuracy_score(y_pred,y_test)*100, 2)) + \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxDPLSQQkmw7",
        "colab_type": "text"
      },
      "source": [
        "## Notes on Gradient Boosting\n",
        "\n",
        "1. Using a small max_features value can significantly decrease the runtime.\n",
        "\n",
        "\n",
        "2. The core principle of AdaBoost is to fit a sequence of weak learners \n",
        "(i.e., models that are only slightlybetter than random guessing, such as small decision trees) on repeatedly modified versions of the data.\n",
        "\n",
        "\n",
        "3. The predictions from all of them are then combined through a weighted \n",
        "majority vote (or sum) to produce the final prediction.\n",
        "\n",
        "\n",
        "4. GradientBoostingClassifier supports both binary and multi-class classification\n",
        "\n",
        "\n",
        "5. The number of weak learners (i.e. regression trees) is controlled by the parameter n_estimators; The size of each tree can be controlled either by setting the tree depth via max_depth or by setting the number of leaf nodes via max_leaf_nodes. The learning_rate is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage .\n",
        "\n",
        "\n",
        "6. Both GradientBoostingRegressor and GradientBoostingClassifier support warm_start=True which allows you to add more estimators to an already fitted model.\n",
        "\n",
        "\n",
        "7. Scikit-learn 0.21 introduced two new experimental implementations of gradient boosting trees, namely HistGradientBoostingClassifier and HistGradientBoostingRegressor, inspired by LightGBM (See [LightGBM]).\n",
        "\n",
        "\n",
        "8. These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples.\n",
        "\n",
        "\n",
        "9. They also have built-in support for missing values, which avoids the need for an imputer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrMufmAykmw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "276f9768-2bd2-4d6a-85f8-a297e10ce093"
      },
      "source": [
        "'''\n",
        "check for the following algos\n",
        "\n",
        "1. LightGBM\n",
        "2. XGBoost\n",
        "3. HistGradientBoostingClassifier\n",
        "4. \n",
        "'''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncheck for the following algos\\n\\n1. LightGBM\\n2. XGBoost\\n3. HistGradientBoostingClassifier\\n4. \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}